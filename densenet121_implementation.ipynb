{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7139ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18189052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76838670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cifar10 data\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1], X_train.shape[2], X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70820e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing  (0 to 1)\n",
    "# https://stackoverflow.com/questions/62783984/how-to-normalize-pixel-values-in-an-image-and-save-it\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf8fd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac2abab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d0c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False, padding='same')(relu)\n",
    "        if dropout_rate > 0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "    return temp\n",
    "\n",
    "# Transition Block\n",
    "\n",
    "def transition(input, num_filter=12, dropout_rate=0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression),(1,1),use_bias=False, padding='same')(relu)\n",
    "    if dropout_rate > 0:\n",
    "        Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "# Output layer\n",
    "\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41205f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filter = 12\n",
    "dropout_rate = 0\n",
    "l = 12\n",
    "\n",
    "input = layers.Input(shape=(img_height, img_width, channel))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False, padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, 6, dropout_rate)\n",
    "First_Transition = transition(First_Block, 128, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, 12, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, 128, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, 24, dropout_rate)\n",
    "Third_Transition = transition(Third_Block,128, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  16, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56f5239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 12)   324         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 12)  48          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 12)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 3)    324         ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 15)   0           ['conv2d[0][0]',                 \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 15)  60          ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 15)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 3)    405         ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 18)   0           ['concatenate[0][0]',            \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 18)  72          ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 18)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 3)    486         ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 21)   0           ['concatenate_1[0][0]',          \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 21)  84          ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 21)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 3)    567         ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 24)   0           ['concatenate_2[0][0]',          \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 24)  96          ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 24)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 3)    648         ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 27)   0           ['concatenate_3[0][0]',          \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 27)  108         ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 27)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 3)    729         ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 30)   0           ['concatenate_4[0][0]',          \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 30)  120         ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 30)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 3)    810         ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 33)   0           ['concatenate_5[0][0]',          \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 33)  132         ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 33)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 3)    891         ['activation_7[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 32, 32, 36)   0           ['concatenate_6[0][0]',          \n",
      "                                                                  'conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 36)  144         ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 36)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 3)    972         ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 32, 32, 39)   0           ['concatenate_7[0][0]',          \n",
      "                                                                  'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 39)  156         ['concatenate_8[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 39)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 3)    1053        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 42)   0           ['concatenate_8[0][0]',          \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 42)  168         ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 42)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 3)    1134        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 45)   0           ['concatenate_9[0][0]',          \n",
      "                                                                  'conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 45)  180         ['concatenate_10[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 45)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 3)    1215        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 48)   0           ['concatenate_10[0][0]',         \n",
      "                                                                  'conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 48)  192         ['concatenate_11[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 48)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 64)   3072        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 64)  0           ['conv2d_13[0][0]']              \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 64)  256         ['average_pooling2d[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 6)    3456        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 16, 16, 70)   0           ['average_pooling2d[0][0]',      \n",
      "                                                                  'conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 70)  280         ['concatenate_12[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 70)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 6)    3780        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 16, 16, 76)   0           ['concatenate_12[0][0]',         \n",
      "                                                                  'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 76)  304         ['concatenate_13[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 76)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 6)    4104        ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 16, 16, 82)   0           ['concatenate_13[0][0]',         \n",
      "                                                                  'conv2d_16[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 82)  328         ['concatenate_14[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 16, 82)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 6)    4428        ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 16, 16, 88)   0           ['concatenate_14[0][0]',         \n",
      "                                                                  'conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 88)  352         ['concatenate_15[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 16, 16, 88)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 6)    4752        ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 94)   0           ['concatenate_15[0][0]',         \n",
      "                                                                  'conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 94)  376         ['concatenate_16[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 16, 16, 94)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 6)    5076        ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 16, 16, 100)  0           ['concatenate_16[0][0]',         \n",
      "                                                                  'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 100)  400        ['concatenate_17[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 16, 100)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 6)    5400        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 16, 16, 106)  0           ['concatenate_17[0][0]',         \n",
      "                                                                  'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 106)  424        ['concatenate_18[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 16, 16, 106)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 6)    5724        ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 16, 16, 112)  0           ['concatenate_18[0][0]',         \n",
      "                                                                  'conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 112)  448        ['concatenate_19[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 16, 16, 112)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 6)    6048        ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 16, 16, 118)  0           ['concatenate_19[0][0]',         \n",
      "                                                                  'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 118)  472        ['concatenate_20[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 16, 16, 118)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 6)    6372        ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 16, 16, 124)  0           ['concatenate_20[0][0]',         \n",
      "                                                                  'conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 124)  496        ['concatenate_21[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 124)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 6)    6696        ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 16, 16, 130)  0           ['concatenate_21[0][0]',         \n",
      "                                                                  'conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 130)  520        ['concatenate_22[0][0]']         \n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 130)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 6)    7020        ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 16, 16, 136)  0           ['concatenate_22[0][0]',         \n",
      "                                                                  'conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 136)  544        ['concatenate_23[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 136)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 64)   8704        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 8, 8, 64)    0           ['conv2d_26[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 64)    256         ['average_pooling2d_1[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 8, 8, 12)     6912        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 8, 8, 76)     0           ['average_pooling2d_1[0][0]',    \n",
      "                                                                  'conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 8, 8, 76)    304         ['concatenate_24[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 8, 8, 76)     0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 8, 8, 12)     8208        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 8, 8, 88)     0           ['concatenate_24[0][0]',         \n",
      "                                                                  'conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 8, 8, 88)    352         ['concatenate_25[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 8, 8, 88)     0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 8, 8, 12)     9504        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 8, 8, 100)    0           ['concatenate_25[0][0]',         \n",
      "                                                                  'conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 8, 8, 100)   400         ['concatenate_26[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 8, 8, 100)    0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 12)     10800       ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 8, 8, 112)    0           ['concatenate_26[0][0]',         \n",
      "                                                                  'conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 112)   448         ['concatenate_27[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 8, 8, 112)    0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 12)     12096       ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 8, 8, 124)    0           ['concatenate_27[0][0]',         \n",
      "                                                                  'conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 124)   496         ['concatenate_28[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 8, 8, 124)    0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 12)     13392       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 8, 8, 136)    0           ['concatenate_28[0][0]',         \n",
      "                                                                  'conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 136)   544         ['concatenate_29[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 136)    0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_33 (Conv2D)             (None, 8, 8, 12)     14688       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 8, 8, 148)    0           ['concatenate_29[0][0]',         \n",
      "                                                                  'conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 148)   592         ['concatenate_30[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 148)    0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 12)     15984       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 8, 8, 160)    0           ['concatenate_30[0][0]',         \n",
      "                                                                  'conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 160)   640         ['concatenate_31[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 160)    0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 12)     17280       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 8, 8, 172)    0           ['concatenate_31[0][0]',         \n",
      "                                                                  'conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 172)   688         ['concatenate_32[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 172)    0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 12)     18576       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 8, 8, 184)    0           ['concatenate_32[0][0]',         \n",
      "                                                                  'conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 184)   736         ['concatenate_33[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 184)    0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 12)     19872       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 8, 8, 196)    0           ['concatenate_33[0][0]',         \n",
      "                                                                  'conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 196)   784         ['concatenate_34[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 196)    0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 12)     21168       ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 8, 8, 208)    0           ['concatenate_34[0][0]',         \n",
      "                                                                  'conv2d_38[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 208)   832         ['concatenate_35[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 208)    0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     13312       ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 4, 4, 64)    0           ['conv2d_39[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 4, 4, 64)    256         ['average_pooling2d_2[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 4, 4, 64)     0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 4, 4, 8)      4608        ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenate)   (None, 4, 4, 72)     0           ['average_pooling2d_2[0][0]',    \n",
      "                                                                  'conv2d_40[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 4, 4, 72)    288         ['concatenate_36[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 4, 4, 72)     0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 4, 4, 8)      5184        ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_37 (Concatenate)   (None, 4, 4, 80)     0           ['concatenate_36[0][0]',         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 4, 4, 80)    320         ['concatenate_37[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 4, 4, 80)     0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 4, 4, 8)      5760        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_38 (Concatenate)   (None, 4, 4, 88)     0           ['concatenate_37[0][0]',         \n",
      "                                                                  'conv2d_42[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 4, 4, 88)    352         ['concatenate_38[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 4, 4, 88)     0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 4, 4, 8)      6336        ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_39 (Concatenate)   (None, 4, 4, 96)     0           ['concatenate_38[0][0]',         \n",
      "                                                                  'conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 4, 4, 96)    384         ['concatenate_39[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 4, 4, 96)     0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 4, 4, 8)      6912        ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenate)   (None, 4, 4, 104)    0           ['concatenate_39[0][0]',         \n",
      "                                                                  'conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 4, 4, 104)   416         ['concatenate_40[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 4, 4, 104)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 4, 4, 8)      7488        ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenate)   (None, 4, 4, 112)    0           ['concatenate_40[0][0]',         \n",
      "                                                                  'conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 4, 4, 112)   448         ['concatenate_41[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 4, 4, 112)    0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 4, 4, 8)      8064        ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_42 (Concatenate)   (None, 4, 4, 120)    0           ['concatenate_41[0][0]',         \n",
      "                                                                  'conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 4, 4, 120)   480         ['concatenate_42[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 4, 4, 120)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 4, 4, 8)      8640        ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_43 (Concatenate)   (None, 4, 4, 128)    0           ['concatenate_42[0][0]',         \n",
      "                                                                  'conv2d_47[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 4, 4, 128)   512         ['concatenate_43[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 4, 4, 8)      9216        ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_44 (Concatenate)   (None, 4, 4, 136)    0           ['concatenate_43[0][0]',         \n",
      "                                                                  'conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 4, 4, 136)   544         ['concatenate_44[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 4, 4, 136)    0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 4, 4, 8)      9792        ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 4, 4, 144)    0           ['concatenate_44[0][0]',         \n",
      "                                                                  'conv2d_49[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 4, 4, 144)   576         ['concatenate_45[0][0]']         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 4, 4, 144)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 4, 4, 8)      10368       ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 4, 4, 152)    0           ['concatenate_45[0][0]',         \n",
      "                                                                  'conv2d_50[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 4, 4, 152)   608         ['concatenate_46[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 4, 4, 152)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 4, 4, 8)      10944       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenate)   (None, 4, 4, 160)    0           ['concatenate_46[0][0]',         \n",
      "                                                                  'conv2d_51[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 4, 4, 160)   640         ['concatenate_47[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 4, 4, 160)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 2, 2, 160)   0           ['activation_51[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 640)          0           ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           6410        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 385,360\n",
      "Trainable params: 375,532\n",
      "Non-trainable params: 9,828\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "449486bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ddd5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the loss function and the optimizer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb8f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model1.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "778d79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Augmentation\n",
    "# https://medium.com/analytics-vidhya/image-augmentation-9b7be3972e27\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             rotation_range=10,\n",
    "                             shear_range=0.1,\n",
    "                             zoom_range=0.1,\n",
    "                             horizontal_flip=True)\n",
    "\n",
    "iterator_train = datagen.flow(X_train, y_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38fcaf2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.3646 - accuracy: 0.5060\n",
      "Epoch 1: loss improved from inf to 1.36457, saving model to model1.h5\n",
      "782/782 [==============================] - 169s 198ms/step - loss: 1.3646 - accuracy: 0.5060 - val_loss: 1.2539 - val_accuracy: 0.5741\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.6736\n",
      "Epoch 2: loss improved from 1.36457 to 0.92142, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.9214 - accuracy: 0.6736 - val_loss: 0.8965 - val_accuracy: 0.6858\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.7736 - accuracy: 0.7285\n",
      "Epoch 3: loss improved from 0.92142 to 0.77363, saving model to model1.h5\n",
      "782/782 [==============================] - 111s 142ms/step - loss: 0.7736 - accuracy: 0.7285 - val_loss: 1.0863 - val_accuracy: 0.6513\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.7613\n",
      "Epoch 4: loss improved from 0.77363 to 0.67975, saving model to model1.h5\n",
      "782/782 [==============================] - 107s 136ms/step - loss: 0.6798 - accuracy: 0.7613 - val_loss: 0.9259 - val_accuracy: 0.6812\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.7803\n",
      "Epoch 5: loss improved from 0.67975 to 0.62548, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 139ms/step - loss: 0.6255 - accuracy: 0.7803 - val_loss: 0.6910 - val_accuracy: 0.7654\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.5737 - accuracy: 0.8012\n",
      "Epoch 6: loss improved from 0.62548 to 0.57365, saving model to model1.h5\n",
      "782/782 [==============================] - 116s 148ms/step - loss: 0.5737 - accuracy: 0.8012 - val_loss: 0.7816 - val_accuracy: 0.7459\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.8176\n",
      "Epoch 7: loss improved from 0.57365 to 0.52850, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.5285 - accuracy: 0.8176 - val_loss: 0.7337 - val_accuracy: 0.7559\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4993 - accuracy: 0.8256\n",
      "Epoch 8: loss improved from 0.52850 to 0.49927, saving model to model1.h5\n",
      "782/782 [==============================] - 114s 145ms/step - loss: 0.4993 - accuracy: 0.8256 - val_loss: 0.7623 - val_accuracy: 0.7572\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4740 - accuracy: 0.8344\n",
      "Epoch 9: loss improved from 0.49927 to 0.47398, saving model to model1.h5\n",
      "782/782 [==============================] - 106s 136ms/step - loss: 0.4740 - accuracy: 0.8344 - val_loss: 0.6917 - val_accuracy: 0.7697\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.8412\n",
      "Epoch 10: loss improved from 0.47398 to 0.45501, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.4550 - accuracy: 0.8412 - val_loss: 0.5803 - val_accuracy: 0.8043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2780e1a90d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(iterator_train, batch_size=256, epochs=10, callbacks=callbacks_list,verbose=1,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4f7999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 26ms/step - loss: 0.5803 - accuracy: 0.8043\n",
      "Test loss: 0.580264687538147\n",
      "Test accuracy: 0.8043000102043152\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c365538",
   "metadata": {},
   "source": [
    "- For 10 epochs, we got the Test Accuaracy of 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d750eb20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4404 - accuracy: 0.8460\n",
      "Epoch 1: loss improved from 0.45501 to 0.44041, saving model to model1.h5\n",
      "782/782 [==============================] - 115s 141ms/step - loss: 0.4404 - accuracy: 0.8460 - val_loss: 0.5821 - val_accuracy: 0.8056\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4146 - accuracy: 0.8542\n",
      "Epoch 2: loss improved from 0.44041 to 0.41462, saving model to model1.h5\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.4146 - accuracy: 0.8542 - val_loss: 0.6521 - val_accuracy: 0.7968\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.8611\n",
      "Epoch 3: loss improved from 0.41462 to 0.40116, saving model to model1.h5\n",
      "782/782 [==============================] - 110s 140ms/step - loss: 0.4012 - accuracy: 0.8611 - val_loss: 0.5612 - val_accuracy: 0.8192\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3803 - accuracy: 0.8661\n",
      "Epoch 4: loss improved from 0.40116 to 0.38028, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.3803 - accuracy: 0.8661 - val_loss: 0.5730 - val_accuracy: 0.8092\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3685 - accuracy: 0.8695\n",
      "Epoch 5: loss improved from 0.38028 to 0.36847, saving model to model1.h5\n",
      "782/782 [==============================] - 109s 140ms/step - loss: 0.3685 - accuracy: 0.8695 - val_loss: 0.6002 - val_accuracy: 0.8104\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3554 - accuracy: 0.8758\n",
      "Epoch 6: loss improved from 0.36847 to 0.35543, saving model to model1.h5\n",
      "782/782 [==============================] - 110s 141ms/step - loss: 0.3554 - accuracy: 0.8758 - val_loss: 0.4713 - val_accuracy: 0.8454\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8794\n",
      "Epoch 7: loss improved from 0.35543 to 0.34542, saving model to model1.h5\n",
      "782/782 [==============================] - 112s 143ms/step - loss: 0.3454 - accuracy: 0.8794 - val_loss: 0.4979 - val_accuracy: 0.8361\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3339 - accuracy: 0.8832\n",
      "Epoch 8: loss improved from 0.34542 to 0.33394, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 139ms/step - loss: 0.3339 - accuracy: 0.8832 - val_loss: 0.5643 - val_accuracy: 0.8194\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3304 - accuracy: 0.8843\n",
      "Epoch 9: loss improved from 0.33394 to 0.33037, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.3304 - accuracy: 0.8843 - val_loss: 0.6016 - val_accuracy: 0.8026\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8902\n",
      "Epoch 10: loss improved from 0.33037 to 0.31023, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.3102 - accuracy: 0.8902 - val_loss: 0.5631 - val_accuracy: 0.8295\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.8930\n",
      "Epoch 11: loss improved from 0.31023 to 0.30454, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.3045 - accuracy: 0.8930 - val_loss: 0.5289 - val_accuracy: 0.8387\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.8972\n",
      "Epoch 12: loss improved from 0.30454 to 0.29405, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.2941 - accuracy: 0.8972 - val_loss: 0.4838 - val_accuracy: 0.8432\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2889 - accuracy: 0.8995\n",
      "Epoch 13: loss improved from 0.29405 to 0.28888, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.2889 - accuracy: 0.8995 - val_loss: 0.5302 - val_accuracy: 0.8325\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9007\n",
      "Epoch 14: loss improved from 0.28888 to 0.28468, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.2847 - accuracy: 0.9007 - val_loss: 0.4247 - val_accuracy: 0.8603\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.9014\n",
      "Epoch 15: loss improved from 0.28468 to 0.27690, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.2769 - accuracy: 0.9014 - val_loss: 0.4977 - val_accuracy: 0.8431\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.9047\n",
      "Epoch 16: loss improved from 0.27690 to 0.26982, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.2698 - accuracy: 0.9047 - val_loss: 0.4665 - val_accuracy: 0.8531\n",
      "Epoch 17/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.9086\n",
      "Epoch 17: loss improved from 0.26982 to 0.26420, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.2642 - accuracy: 0.9086 - val_loss: 0.4193 - val_accuracy: 0.8641\n",
      "Epoch 18/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9100\n",
      "Epoch 18: loss improved from 0.26420 to 0.25799, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.2580 - accuracy: 0.9100 - val_loss: 0.6375 - val_accuracy: 0.8085\n",
      "Epoch 19/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.9130\n",
      "Epoch 19: loss improved from 0.25799 to 0.24725, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.2473 - accuracy: 0.9130 - val_loss: 0.4500 - val_accuracy: 0.8563\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9161\n",
      "Epoch 20: loss improved from 0.24725 to 0.24512, saving model to model1.h5\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.2451 - accuracy: 0.9161 - val_loss: 0.5419 - val_accuracy: 0.8373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27924f769a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the previously trained model and continuing to train to imporove test accuracy.\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model1.h5')\n",
    "\n",
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# this time, we will train for 20 epochs\n",
    "\n",
    "model.fit(iterator_train, batch_size=256, epochs=20, callbacks=callbacks_list,validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a411d65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step - loss: 0.5419 - accuracy: 0.8373\n",
      "Test loss: 0.5418946146965027\n",
      "Test accuracy: 0.8373000025749207\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd7103",
   "metadata": {},
   "source": [
    "- After retraining the same model for 20 epochs, we got the test accuracy of 83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13b30929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9134\n",
      "Epoch 1: loss improved from 0.24512 to 0.24347, saving model to model1.h5\n",
      "782/782 [==============================] - 111s 136ms/step - loss: 0.2435 - accuracy: 0.9134 - val_loss: 0.4518 - val_accuracy: 0.8609\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2348 - accuracy: 0.9168\n",
      "Epoch 2: loss improved from 0.24347 to 0.23481, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 139ms/step - loss: 0.2348 - accuracy: 0.9168 - val_loss: 0.4549 - val_accuracy: 0.8574\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.9167\n",
      "Epoch 3: loss improved from 0.23481 to 0.23378, saving model to model1.h5\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 0.2338 - accuracy: 0.9167 - val_loss: 0.5030 - val_accuracy: 0.8517\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.9188\n",
      "Epoch 4: loss improved from 0.23378 to 0.22860, saving model to model1.h5\n",
      "782/782 [==============================] - 112s 143ms/step - loss: 0.2286 - accuracy: 0.9188 - val_loss: 0.4798 - val_accuracy: 0.8541\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9203\n",
      "Epoch 5: loss improved from 0.22860 to 0.22272, saving model to model1.h5\n",
      "782/782 [==============================] - 114s 146ms/step - loss: 0.2227 - accuracy: 0.9203 - val_loss: 0.5116 - val_accuracy: 0.8493\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9229\n",
      "Epoch 6: loss improved from 0.22272 to 0.21833, saving model to model1.h5\n",
      "782/782 [==============================] - 109s 140ms/step - loss: 0.2183 - accuracy: 0.9229 - val_loss: 0.4700 - val_accuracy: 0.8557\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9243\n",
      "Epoch 7: loss improved from 0.21833 to 0.21463, saving model to model1.h5\n",
      "782/782 [==============================] - 109s 140ms/step - loss: 0.2146 - accuracy: 0.9243 - val_loss: 0.3942 - val_accuracy: 0.8786\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9262\n",
      "Epoch 8: loss improved from 0.21463 to 0.21037, saving model to model1.h5\n",
      "782/782 [==============================] - 116s 149ms/step - loss: 0.2104 - accuracy: 0.9262 - val_loss: 0.6285 - val_accuracy: 0.8350\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9276\n",
      "Epoch 9: loss improved from 0.21037 to 0.20297, saving model to model1.h5\n",
      "782/782 [==============================] - 124s 158ms/step - loss: 0.2030 - accuracy: 0.9276 - val_loss: 0.5203 - val_accuracy: 0.8505\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2010 - accuracy: 0.9296\n",
      "Epoch 10: loss improved from 0.20297 to 0.20102, saving model to model1.h5\n",
      "782/782 [==============================] - 117s 149ms/step - loss: 0.2010 - accuracy: 0.9296 - val_loss: 0.4466 - val_accuracy: 0.8650\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9302\n",
      "Epoch 11: loss improved from 0.20102 to 0.19844, saving model to model1.h5\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.1984 - accuracy: 0.9302 - val_loss: 0.4458 - val_accuracy: 0.8703\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9316\n",
      "Epoch 12: loss improved from 0.19844 to 0.19572, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.1957 - accuracy: 0.9316 - val_loss: 0.4327 - val_accuracy: 0.8741\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9320\n",
      "Epoch 13: loss improved from 0.19572 to 0.19397, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.1940 - accuracy: 0.9320 - val_loss: 0.4946 - val_accuracy: 0.8578\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9335\n",
      "Epoch 14: loss improved from 0.19397 to 0.18736, saving model to model1.h5\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.1874 - accuracy: 0.9335 - val_loss: 0.5032 - val_accuracy: 0.8555\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9336\n",
      "Epoch 15: loss improved from 0.18736 to 0.18716, saving model to model1.h5\n",
      "782/782 [==============================] - 107s 136ms/step - loss: 0.1872 - accuracy: 0.9336 - val_loss: 0.5748 - val_accuracy: 0.8444\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9350\n",
      "Epoch 16: loss improved from 0.18716 to 0.18646, saving model to model1.h5\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.1865 - accuracy: 0.9350 - val_loss: 0.4703 - val_accuracy: 0.8647\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1806 - accuracy: 0.9348\n",
      "Epoch 17: loss improved from 0.18646 to 0.18060, saving model to model1.h5\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.1806 - accuracy: 0.9348 - val_loss: 0.5009 - val_accuracy: 0.8628\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9384\n",
      "Epoch 18: loss improved from 0.18060 to 0.17749, saving model to model1.h5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.1775 - accuracy: 0.9384 - val_loss: 0.4148 - val_accuracy: 0.8810\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1745 - accuracy: 0.9393\n",
      "Epoch 19: loss improved from 0.17749 to 0.17447, saving model to model1.h5\n",
      "782/782 [==============================] - 107s 136ms/step - loss: 0.1745 - accuracy: 0.9393 - val_loss: 0.4560 - val_accuracy: 0.8658\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9378\n",
      "Epoch 20: loss improved from 0.17447 to 0.17334, saving model to model1.h5\n",
      "782/782 [==============================] - 111s 141ms/step - loss: 0.1733 - accuracy: 0.9378 - val_loss: 0.4639 - val_accuracy: 0.8725\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9392\n",
      "Epoch 21: loss improved from 0.17334 to 0.17312, saving model to model1.h5\n",
      "782/782 [==============================] - 110s 140ms/step - loss: 0.1731 - accuracy: 0.9392 - val_loss: 0.6292 - val_accuracy: 0.8317\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9408\n",
      "Epoch 22: loss improved from 0.17312 to 0.16872, saving model to model1.h5\n",
      "782/782 [==============================] - 106s 135ms/step - loss: 0.1687 - accuracy: 0.9408 - val_loss: 0.5668 - val_accuracy: 0.8480\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9415\n",
      "Epoch 23: loss improved from 0.16872 to 0.16296, saving model to model1.h5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.1630 - accuracy: 0.9415 - val_loss: 0.4446 - val_accuracy: 0.8703\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9428\n",
      "Epoch 24: loss improved from 0.16296 to 0.16095, saving model to model1.h5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.1610 - accuracy: 0.9428 - val_loss: 0.4389 - val_accuracy: 0.8741\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9428\n",
      "Epoch 25: loss did not improve from 0.16095\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1621 - accuracy: 0.9428 - val_loss: 0.4647 - val_accuracy: 0.8701\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1588 - accuracy: 0.9429\n",
      "Epoch 26: loss improved from 0.16095 to 0.15878, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1588 - accuracy: 0.9429 - val_loss: 0.5032 - val_accuracy: 0.8584\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9444\n",
      "Epoch 27: loss improved from 0.15878 to 0.15702, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1570 - accuracy: 0.9444 - val_loss: 0.5710 - val_accuracy: 0.8523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1587 - accuracy: 0.9436\n",
      "Epoch 28: loss did not improve from 0.15702\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1587 - accuracy: 0.9436 - val_loss: 0.4504 - val_accuracy: 0.8726\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9461\n",
      "Epoch 29: loss improved from 0.15702 to 0.15243, saving model to model1.h5\n",
      "782/782 [==============================] - 106s 135ms/step - loss: 0.1524 - accuracy: 0.9461 - val_loss: 0.4835 - val_accuracy: 0.8708\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9464\n",
      "Epoch 30: loss improved from 0.15243 to 0.14910, saving model to model1.h5\n",
      "782/782 [==============================] - 106s 135ms/step - loss: 0.1491 - accuracy: 0.9464 - val_loss: 0.4613 - val_accuracy: 0.8738\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9472\n",
      "Epoch 31: loss improved from 0.14910 to 0.14791, saving model to model1.h5\n",
      "782/782 [==============================] - 106s 135ms/step - loss: 0.1479 - accuracy: 0.9472 - val_loss: 0.4451 - val_accuracy: 0.8773\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9467\n",
      "Epoch 32: loss did not improve from 0.14791\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.1480 - accuracy: 0.9467 - val_loss: 0.4260 - val_accuracy: 0.8767\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9476\n",
      "Epoch 33: loss did not improve from 0.14791\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.1484 - accuracy: 0.9476 - val_loss: 0.5302 - val_accuracy: 0.8611\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9495\n",
      "Epoch 34: loss improved from 0.14791 to 0.14393, saving model to model1.h5\n",
      "782/782 [==============================] - 106s 135ms/step - loss: 0.1439 - accuracy: 0.9495 - val_loss: 0.5701 - val_accuracy: 0.8464\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9484\n",
      "Epoch 35: loss improved from 0.14393 to 0.14310, saving model to model1.h5\n",
      "782/782 [==============================] - 105s 135ms/step - loss: 0.1431 - accuracy: 0.9484 - val_loss: 0.5203 - val_accuracy: 0.8603\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9483\n",
      "Epoch 36: loss did not improve from 0.14310\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.1444 - accuracy: 0.9483 - val_loss: 0.4437 - val_accuracy: 0.8765\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9508\n",
      "Epoch 37: loss improved from 0.14310 to 0.13805, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.1381 - accuracy: 0.9508 - val_loss: 0.4772 - val_accuracy: 0.8702\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9513\n",
      "Epoch 38: loss improved from 0.13805 to 0.13440, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.1344 - accuracy: 0.9513 - val_loss: 0.4406 - val_accuracy: 0.8779\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9516\n",
      "Epoch 39: loss did not improve from 0.13440\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.1346 - accuracy: 0.9516 - val_loss: 0.5156 - val_accuracy: 0.8600\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1343 - accuracy: 0.9524\n",
      "Epoch 40: loss improved from 0.13440 to 0.13427, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.1343 - accuracy: 0.9524 - val_loss: 0.4566 - val_accuracy: 0.8725\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.9529\n",
      "Epoch 41: loss improved from 0.13427 to 0.13250, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.1325 - accuracy: 0.9529 - val_loss: 0.6066 - val_accuracy: 0.8534\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9533\n",
      "Epoch 42: loss improved from 0.13250 to 0.12993, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.1299 - accuracy: 0.9533 - val_loss: 0.5058 - val_accuracy: 0.8649\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9525\n",
      "Epoch 43: loss did not improve from 0.12993\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.1305 - accuracy: 0.9525 - val_loss: 0.4655 - val_accuracy: 0.8830\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9540\n",
      "Epoch 44: loss improved from 0.12993 to 0.12700, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1270 - accuracy: 0.9540 - val_loss: 0.6103 - val_accuracy: 0.8496\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9535\n",
      "Epoch 45: loss did not improve from 0.12700\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1282 - accuracy: 0.9535 - val_loss: 0.5119 - val_accuracy: 0.8722\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9549\n",
      "Epoch 46: loss did not improve from 0.12700\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1293 - accuracy: 0.9549 - val_loss: 0.4557 - val_accuracy: 0.8783\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9545\n",
      "Epoch 47: loss improved from 0.12700 to 0.12624, saving model to model1.h5\n",
      "782/782 [==============================] - 105s 135ms/step - loss: 0.1262 - accuracy: 0.9545 - val_loss: 0.5035 - val_accuracy: 0.8667\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9556\n",
      "Epoch 48: loss improved from 0.12624 to 0.12456, saving model to model1.h5\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.1246 - accuracy: 0.9556 - val_loss: 0.4637 - val_accuracy: 0.8729\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9572\n",
      "Epoch 49: loss improved from 0.12456 to 0.12069, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 137ms/step - loss: 0.1207 - accuracy: 0.9572 - val_loss: 0.5064 - val_accuracy: 0.8720\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9560\n",
      "Epoch 50: loss did not improve from 0.12069\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.1235 - accuracy: 0.9560 - val_loss: 0.4363 - val_accuracy: 0.8860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279e511eac0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the previously trained model and continuing to train to imporove test accuracy.\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model1.h5')\n",
    "\n",
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# this time, we will train for 50 epochs\n",
    "\n",
    "model.fit(iterator_train, batch_size=256, epochs=50, callbacks=callbacks_list,validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1ace13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step - loss: 0.4363 - accuracy: 0.8860\n",
      "Test loss: 0.43628519773483276\n",
      "Test accuracy: 0.8859999775886536\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d735d5",
   "metadata": {},
   "source": [
    "- After retraining the same model for 50 more epochs, we got the test accuracy of 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5a024c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1224 - accuracy: 0.9558\n",
      "Epoch 1: loss did not improve from 0.12069\n",
      "782/782 [==============================] - 110s 134ms/step - loss: 0.1224 - accuracy: 0.9558 - val_loss: 0.4663 - val_accuracy: 0.8812\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9559\n",
      "Epoch 2: loss did not improve from 0.12069\n",
      "782/782 [==============================] - 107s 136ms/step - loss: 0.1217 - accuracy: 0.9559 - val_loss: 0.4316 - val_accuracy: 0.8876\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9582\n",
      "Epoch 3: loss improved from 0.12069 to 0.11885, saving model to model1.h5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.1188 - accuracy: 0.9582 - val_loss: 0.4370 - val_accuracy: 0.8832\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9593\n",
      "Epoch 4: loss improved from 0.11885 to 0.11516, saving model to model1.h5\n",
      "782/782 [==============================] - 109s 139ms/step - loss: 0.1152 - accuracy: 0.9593 - val_loss: 0.5615 - val_accuracy: 0.8653\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9595\n",
      "Epoch 5: loss improved from 0.11516 to 0.11484, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 139ms/step - loss: 0.1148 - accuracy: 0.9595 - val_loss: 0.4523 - val_accuracy: 0.8875\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9582\n",
      "Epoch 6: loss did not improve from 0.11484\n",
      "782/782 [==============================] - 108s 139ms/step - loss: 0.1180 - accuracy: 0.9582 - val_loss: 0.4883 - val_accuracy: 0.8783\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9583\n",
      "Epoch 7: loss did not improve from 0.11484\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1179 - accuracy: 0.9583 - val_loss: 0.4893 - val_accuracy: 0.8727\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1137 - accuracy: 0.9593\n",
      "Epoch 8: loss improved from 0.11484 to 0.11366, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1137 - accuracy: 0.9593 - val_loss: 0.4395 - val_accuracy: 0.8869\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9605\n",
      "Epoch 9: loss improved from 0.11366 to 0.11008, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.1101 - accuracy: 0.9605 - val_loss: 0.4758 - val_accuracy: 0.8808\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9600\n",
      "Epoch 10: loss did not improve from 0.11008\n",
      "782/782 [==============================] - 106s 136ms/step - loss: 0.1119 - accuracy: 0.9600 - val_loss: 0.5712 - val_accuracy: 0.8653\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9606\n",
      "Epoch 11: loss improved from 0.11008 to 0.10996, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.1100 - accuracy: 0.9606 - val_loss: 0.4141 - val_accuracy: 0.8899\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9622\n",
      "Epoch 12: loss improved from 0.10996 to 0.10816, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.1082 - accuracy: 0.9622 - val_loss: 0.4390 - val_accuracy: 0.8888\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.9603\n",
      "Epoch 13: loss improved from 0.10816 to 0.10788, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.1079 - accuracy: 0.9603 - val_loss: 0.5153 - val_accuracy: 0.8737\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9604\n",
      "Epoch 14: loss did not improve from 0.10788\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.1108 - accuracy: 0.9604 - val_loss: 0.4806 - val_accuracy: 0.8775\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9624\n",
      "Epoch 15: loss improved from 0.10788 to 0.10767, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.1077 - accuracy: 0.9624 - val_loss: 0.5466 - val_accuracy: 0.8659\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9634\n",
      "Epoch 16: loss improved from 0.10767 to 0.10277, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.1028 - accuracy: 0.9634 - val_loss: 0.4892 - val_accuracy: 0.8808\n",
      "Epoch 17/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9624\n",
      "Epoch 17: loss did not improve from 0.10277\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.1049 - accuracy: 0.9624 - val_loss: 0.4548 - val_accuracy: 0.8858\n",
      "Epoch 18/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1034 - accuracy: 0.9622\n",
      "Epoch 18: loss did not improve from 0.10277\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.1034 - accuracy: 0.9622 - val_loss: 0.5034 - val_accuracy: 0.8769\n",
      "Epoch 19/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9629\n",
      "Epoch 19: loss improved from 0.10277 to 0.10215, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.1021 - accuracy: 0.9629 - val_loss: 0.4773 - val_accuracy: 0.8837\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9622\n",
      "Epoch 20: loss did not improve from 0.10215\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.1057 - accuracy: 0.9622 - val_loss: 0.4665 - val_accuracy: 0.8835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279eaf2cf10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the previously trained model and continuing to train to imporove test accuracy.\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model1.h5')\n",
    "\n",
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# this time, we will train for 20 epochs\n",
    "\n",
    "model.fit(iterator_train, batch_size=256, epochs=20, callbacks=callbacks_list,validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a67b0193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step - loss: 0.4665 - accuracy: 0.8835\n",
      "Test loss: 0.46650537848472595\n",
      "Test accuracy: 0.8834999799728394\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681f0e3",
   "metadata": {},
   "source": [
    "- After retraining the same model for 20 more epochs, we got the test accuracy of 88%\n",
    "- We have trained the model for 100 epochs in total until now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c3d9fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9632\n",
      "Epoch 1: loss improved from inf to 0.10392, saving model to model1.h5\n",
      "782/782 [==============================] - 119s 135ms/step - loss: 0.1039 - accuracy: 0.9632 - val_loss: 0.5125 - val_accuracy: 0.8803\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9635\n",
      "Epoch 2: loss improved from 0.10392 to 0.10308, saving model to model1.h5\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.1031 - accuracy: 0.9635 - val_loss: 0.4477 - val_accuracy: 0.8852\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9636\n",
      "Epoch 3: loss improved from 0.10308 to 0.10007, saving model to model1.h5\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.1001 - accuracy: 0.9636 - val_loss: 0.5503 - val_accuracy: 0.8661\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9623\n",
      "Epoch 4: loss did not improve from 0.10007\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.1024 - accuracy: 0.9623 - val_loss: 0.4990 - val_accuracy: 0.8821\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9629\n",
      "Epoch 5: loss did not improve from 0.10007\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.1024 - accuracy: 0.9629 - val_loss: 0.4806 - val_accuracy: 0.8825\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9635\n",
      "Epoch 6: loss did not improve from 0.10007\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.1018 - accuracy: 0.9635 - val_loss: 0.5316 - val_accuracy: 0.8740\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9649\n",
      "Epoch 7: loss improved from 0.10007 to 0.09919, saving model to model1.h5\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.0992 - accuracy: 0.9649 - val_loss: 0.4960 - val_accuracy: 0.8765\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9657\n",
      "Epoch 8: loss improved from 0.09919 to 0.09845, saving model to model1.h5\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.0984 - accuracy: 0.9657 - val_loss: 0.5458 - val_accuracy: 0.8700\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9650\n",
      "Epoch 9: loss improved from 0.09845 to 0.09588, saving model to model1.h5\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.0959 - accuracy: 0.9650 - val_loss: 0.4431 - val_accuracy: 0.8907\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9658\n",
      "Epoch 10: loss did not improve from 0.09588\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.0964 - accuracy: 0.9658 - val_loss: 0.5445 - val_accuracy: 0.8719\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9655\n",
      "Epoch 11: loss did not improve from 0.09588\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.0968 - accuracy: 0.9655 - val_loss: 0.4637 - val_accuracy: 0.8857\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9653\n",
      "Epoch 12: loss improved from 0.09588 to 0.09500, saving model to model1.h5\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.0950 - accuracy: 0.9653 - val_loss: 0.5075 - val_accuracy: 0.8745\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9668\n",
      "Epoch 13: loss improved from 0.09500 to 0.09344, saving model to model1.h5\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.0934 - accuracy: 0.9668 - val_loss: 0.4489 - val_accuracy: 0.8896\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9665\n",
      "Epoch 14: loss did not improve from 0.09344\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0946 - accuracy: 0.9665 - val_loss: 0.5860 - val_accuracy: 0.8660\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9645\n",
      "Epoch 15: loss did not improve from 0.09344\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0960 - accuracy: 0.9645 - val_loss: 0.6271 - val_accuracy: 0.8577\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9671\n",
      "Epoch 16: loss improved from 0.09344 to 0.09178, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.0918 - accuracy: 0.9671 - val_loss: 0.5115 - val_accuracy: 0.8821\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9679\n",
      "Epoch 17: loss improved from 0.09178 to 0.09010, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.0901 - accuracy: 0.9679 - val_loss: 0.5363 - val_accuracy: 0.8711\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9687\n",
      "Epoch 18: loss improved from 0.09010 to 0.08826, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0883 - accuracy: 0.9687 - val_loss: 0.5779 - val_accuracy: 0.8731\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9669\n",
      "Epoch 19: loss did not improve from 0.08826\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0936 - accuracy: 0.9669 - val_loss: 0.5082 - val_accuracy: 0.8812\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9684\n",
      "Epoch 20: loss improved from 0.08826 to 0.08760, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0876 - accuracy: 0.9684 - val_loss: 0.5594 - val_accuracy: 0.8719\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9676\n",
      "Epoch 21: loss did not improve from 0.08760\n",
      "782/782 [==============================] - 1459s 2s/step - loss: 0.0917 - accuracy: 0.9676 - val_loss: 0.5275 - val_accuracy: 0.8770\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9682\n",
      "Epoch 22: loss did not improve from 0.08760\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0888 - accuracy: 0.9682 - val_loss: 0.6023 - val_accuracy: 0.8651\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9691\n",
      "Epoch 23: loss improved from 0.08760 to 0.08672, saving model to model1.h5\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.0867 - accuracy: 0.9691 - val_loss: 0.5178 - val_accuracy: 0.8835\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9682\n",
      "Epoch 24: loss did not improve from 0.08672\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.0883 - accuracy: 0.9682 - val_loss: 0.5578 - val_accuracy: 0.8749\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9693\n",
      "Epoch 25: loss did not improve from 0.08672\n",
      "782/782 [==============================] - 104s 132ms/step - loss: 0.0875 - accuracy: 0.9693 - val_loss: 0.4689 - val_accuracy: 0.8894\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9693\n",
      "Epoch 26: loss improved from 0.08672 to 0.08500, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0850 - accuracy: 0.9693 - val_loss: 0.4869 - val_accuracy: 0.8828\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9685\n",
      "Epoch 27: loss did not improve from 0.08500\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0887 - accuracy: 0.9685 - val_loss: 0.5485 - val_accuracy: 0.8773\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9686\n",
      "Epoch 28: loss did not improve from 0.08500\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0869 - accuracy: 0.9686 - val_loss: 0.4999 - val_accuracy: 0.8825\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9707\n",
      "Epoch 29: loss improved from 0.08500 to 0.08307, saving model to model1.h5\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.0831 - accuracy: 0.9707 - val_loss: 0.6405 - val_accuracy: 0.8550\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9696\n",
      "Epoch 30: loss did not improve from 0.08307\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0871 - accuracy: 0.9696 - val_loss: 0.4459 - val_accuracy: 0.8901\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9700\n",
      "Epoch 31: loss did not improve from 0.08307\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0837 - accuracy: 0.9700 - val_loss: 0.4841 - val_accuracy: 0.8831\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9705\n",
      "Epoch 32: loss improved from 0.08307 to 0.08291, saving model to model1.h5\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.0829 - accuracy: 0.9705 - val_loss: 0.5349 - val_accuracy: 0.8805\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9691\n",
      "Epoch 33: loss did not improve from 0.08291\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.0835 - accuracy: 0.9691 - val_loss: 0.4973 - val_accuracy: 0.8834\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9691\n",
      "Epoch 34: loss did not improve from 0.08291\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.0841 - accuracy: 0.9691 - val_loss: 0.5736 - val_accuracy: 0.8721\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9705\n",
      "Epoch 35: loss improved from 0.08291 to 0.08265, saving model to model1.h5\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.0827 - accuracy: 0.9705 - val_loss: 0.4982 - val_accuracy: 0.8834\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9708\n",
      "Epoch 36: loss improved from 0.08265 to 0.08101, saving model to model1.h5\n",
      "782/782 [==============================] - 105s 135ms/step - loss: 0.0810 - accuracy: 0.9708 - val_loss: 0.5832 - val_accuracy: 0.8729\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9712\n",
      "Epoch 37: loss did not improve from 0.08101\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.0819 - accuracy: 0.9712 - val_loss: 0.5726 - val_accuracy: 0.8707\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9712\n",
      "Epoch 38: loss did not improve from 0.08101\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.0814 - accuracy: 0.9712 - val_loss: 0.4426 - val_accuracy: 0.8951\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9712\n",
      "Epoch 39: loss improved from 0.08101 to 0.07961, saving model to model1.h5\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.0796 - accuracy: 0.9712 - val_loss: 0.5352 - val_accuracy: 0.8778\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9712\n",
      "Epoch 40: loss did not improve from 0.07961\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0803 - accuracy: 0.9712 - val_loss: 0.5193 - val_accuracy: 0.8846\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9719\n",
      "Epoch 41: loss improved from 0.07961 to 0.07943, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0794 - accuracy: 0.9719 - val_loss: 0.4699 - val_accuracy: 0.8853\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9723\n",
      "Epoch 42: loss improved from 0.07943 to 0.07902, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0790 - accuracy: 0.9723 - val_loss: 0.7182 - val_accuracy: 0.8553\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9727\n",
      "Epoch 43: loss improved from 0.07902 to 0.07748, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0775 - accuracy: 0.9727 - val_loss: 0.4552 - val_accuracy: 0.8923\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9728\n",
      "Epoch 44: loss improved from 0.07748 to 0.07640, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0764 - accuracy: 0.9728 - val_loss: 0.5200 - val_accuracy: 0.8827\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9730\n",
      "Epoch 45: loss improved from 0.07640 to 0.07499, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0750 - accuracy: 0.9730 - val_loss: 0.5194 - val_accuracy: 0.8833\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9726\n",
      "Epoch 46: loss did not improve from 0.07499\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0775 - accuracy: 0.9726 - val_loss: 0.5243 - val_accuracy: 0.8794\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9720\n",
      "Epoch 47: loss did not improve from 0.07499\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.0784 - accuracy: 0.9720 - val_loss: 0.6024 - val_accuracy: 0.8699\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9734\n",
      "Epoch 48: loss improved from 0.07499 to 0.07397, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0740 - accuracy: 0.9734 - val_loss: 0.5626 - val_accuracy: 0.8787\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9731\n",
      "Epoch 49: loss did not improve from 0.07397\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0763 - accuracy: 0.9731 - val_loss: 0.6362 - val_accuracy: 0.8693\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9717\n",
      "Epoch 50: loss did not improve from 0.07397\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0768 - accuracy: 0.9717 - val_loss: 0.4913 - val_accuracy: 0.8882\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9724\n",
      "Epoch 51: loss did not improve from 0.07397\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.5310 - val_accuracy: 0.8839\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9729\n",
      "Epoch 52: loss did not improve from 0.07397\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0755 - accuracy: 0.9729 - val_loss: 0.4990 - val_accuracy: 0.8902\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9759\n",
      "Epoch 53: loss improved from 0.07397 to 0.07063, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0706 - accuracy: 0.9759 - val_loss: 0.5464 - val_accuracy: 0.8800\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9740\n",
      "Epoch 54: loss did not improve from 0.07063\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0728 - accuracy: 0.9740 - val_loss: 0.4818 - val_accuracy: 0.8881\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9751\n",
      "Epoch 55: loss improved from 0.07063 to 0.07041, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0704 - accuracy: 0.9751 - val_loss: 0.5357 - val_accuracy: 0.8829\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9741\n",
      "Epoch 56: loss did not improve from 0.07041\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0726 - accuracy: 0.9741 - val_loss: 0.5267 - val_accuracy: 0.8831\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9757\n",
      "Epoch 57: loss improved from 0.07041 to 0.06984, saving model to model1.h5\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0698 - accuracy: 0.9757 - val_loss: 0.4976 - val_accuracy: 0.8899\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9743\n",
      "Epoch 58: loss did not improve from 0.06984\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.0738 - accuracy: 0.9743 - val_loss: 0.6089 - val_accuracy: 0.8701\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9738\n",
      "Epoch 59: loss did not improve from 0.06984\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.0739 - accuracy: 0.9738 - val_loss: 0.5089 - val_accuracy: 0.8822\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9748\n",
      "Epoch 60: loss did not improve from 0.06984\n",
      "782/782 [==============================] - 106s 136ms/step - loss: 0.0718 - accuracy: 0.9748 - val_loss: 0.5063 - val_accuracy: 0.8851\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9737\n",
      "Epoch 61: loss did not improve from 0.06984\n",
      "782/782 [==============================] - 106s 135ms/step - loss: 0.0750 - accuracy: 0.9737 - val_loss: 0.5304 - val_accuracy: 0.8882\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9744\n",
      "Epoch 62: loss did not improve from 0.06984\n",
      "782/782 [==============================] - 110s 141ms/step - loss: 0.0716 - accuracy: 0.9744 - val_loss: 0.7456 - val_accuracy: 0.8519\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9740\n",
      "Epoch 63: loss did not improve from 0.06984\n",
      "782/782 [==============================] - 106s 136ms/step - loss: 0.0734 - accuracy: 0.9740 - val_loss: 0.4583 - val_accuracy: 0.8977\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9757\n",
      "Epoch 64: loss improved from 0.06984 to 0.06981, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0698 - accuracy: 0.9757 - val_loss: 0.4824 - val_accuracy: 0.8895\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9757\n",
      "Epoch 65: loss improved from 0.06981 to 0.06820, saving model to model1.h5\n",
      "782/782 [==============================] - 111s 142ms/step - loss: 0.0682 - accuracy: 0.9757 - val_loss: 0.4655 - val_accuracy: 0.8966\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9744\n",
      "Epoch 66: loss did not improve from 0.06820\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.0716 - accuracy: 0.9744 - val_loss: 0.4998 - val_accuracy: 0.8937\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9749\n",
      "Epoch 67: loss did not improve from 0.06820\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0696 - accuracy: 0.9749 - val_loss: 0.4810 - val_accuracy: 0.8920\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9764\n",
      "Epoch 68: loss improved from 0.06820 to 0.06608, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.0661 - accuracy: 0.9764 - val_loss: 0.5558 - val_accuracy: 0.8800\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9757\n",
      "Epoch 69: loss did not improve from 0.06608\n",
      "782/782 [==============================] - 105s 135ms/step - loss: 0.0696 - accuracy: 0.9757 - val_loss: 0.5035 - val_accuracy: 0.8926\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9762\n",
      "Epoch 70: loss did not improve from 0.06608\n",
      "782/782 [==============================] - 101s 129ms/step - loss: 0.0687 - accuracy: 0.9762 - val_loss: 0.4808 - val_accuracy: 0.8903\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9761\n",
      "Epoch 71: loss did not improve from 0.06608\n",
      "782/782 [==============================] - 101s 129ms/step - loss: 0.0688 - accuracy: 0.9761 - val_loss: 0.5387 - val_accuracy: 0.8805\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9752\n",
      "Epoch 72: loss did not improve from 0.06608\n",
      "782/782 [==============================] - 101s 129ms/step - loss: 0.0697 - accuracy: 0.9752 - val_loss: 0.5739 - val_accuracy: 0.8789\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9768\n",
      "Epoch 73: loss improved from 0.06608 to 0.06534, saving model to model1.h5\n",
      "782/782 [==============================] - 101s 129ms/step - loss: 0.0653 - accuracy: 0.9768 - val_loss: 0.4940 - val_accuracy: 0.8908\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9750\n",
      "Epoch 74: loss did not improve from 0.06534\n",
      "782/782 [==============================] - 105s 135ms/step - loss: 0.0703 - accuracy: 0.9750 - val_loss: 0.5393 - val_accuracy: 0.8815\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9770\n",
      "Epoch 75: loss improved from 0.06534 to 0.06501, saving model to model1.h5\n",
      "782/782 [==============================] - 106s 135ms/step - loss: 0.0650 - accuracy: 0.9770 - val_loss: 0.5349 - val_accuracy: 0.8855\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9775\n",
      "Epoch 76: loss improved from 0.06501 to 0.06421, saving model to model1.h5\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.0642 - accuracy: 0.9775 - val_loss: 0.5173 - val_accuracy: 0.8861\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9762\n",
      "Epoch 77: loss did not improve from 0.06421\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.0649 - accuracy: 0.9762 - val_loss: 0.6203 - val_accuracy: 0.8722\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9762\n",
      "Epoch 78: loss did not improve from 0.06421\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.0685 - accuracy: 0.9762 - val_loss: 0.6945 - val_accuracy: 0.8611\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9754\n",
      "Epoch 79: loss did not improve from 0.06421\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.0674 - accuracy: 0.9754 - val_loss: 0.4821 - val_accuracy: 0.8940\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9772\n",
      "Epoch 80: loss did not improve from 0.06421\n",
      "782/782 [==============================] - 106s 136ms/step - loss: 0.0644 - accuracy: 0.9772 - val_loss: 0.5574 - val_accuracy: 0.8803\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9758\n",
      "Epoch 81: loss did not improve from 0.06421\n",
      "782/782 [==============================] - 107s 136ms/step - loss: 0.0664 - accuracy: 0.9758 - val_loss: 0.4981 - val_accuracy: 0.8887\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9786\n",
      "Epoch 82: loss improved from 0.06421 to 0.06093, saving model to model1.h5\n",
      "782/782 [==============================] - 106s 136ms/step - loss: 0.0609 - accuracy: 0.9786 - val_loss: 0.5461 - val_accuracy: 0.8814\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9771\n",
      "Epoch 83: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 101s 129ms/step - loss: 0.0638 - accuracy: 0.9771 - val_loss: 0.5568 - val_accuracy: 0.8848\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9772\n",
      "Epoch 84: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0653 - accuracy: 0.9772 - val_loss: 0.5117 - val_accuracy: 0.8887\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9776\n",
      "Epoch 85: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0638 - accuracy: 0.9776 - val_loss: 0.5937 - val_accuracy: 0.8764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9763\n",
      "Epoch 86: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.0669 - accuracy: 0.9763 - val_loss: 0.5170 - val_accuracy: 0.8870\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9771\n",
      "Epoch 87: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 108s 138ms/step - loss: 0.0650 - accuracy: 0.9771 - val_loss: 0.5510 - val_accuracy: 0.8821\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9786\n",
      "Epoch 88: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 107s 136ms/step - loss: 0.0619 - accuracy: 0.9786 - val_loss: 0.5544 - val_accuracy: 0.8840\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9759\n",
      "Epoch 89: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 106s 135ms/step - loss: 0.0681 - accuracy: 0.9759 - val_loss: 0.5059 - val_accuracy: 0.8923\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9779\n",
      "Epoch 90: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.0637 - accuracy: 0.9779 - val_loss: 0.6223 - val_accuracy: 0.8684\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9774\n",
      "Epoch 91: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0632 - accuracy: 0.9774 - val_loss: 0.4484 - val_accuracy: 0.9023\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9773\n",
      "Epoch 92: loss did not improve from 0.06093\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.0646 - accuracy: 0.9773 - val_loss: 0.4691 - val_accuracy: 0.8983\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9784\n",
      "Epoch 93: loss improved from 0.06093 to 0.06059, saving model to model1.h5\n",
      "782/782 [==============================] - 106s 136ms/step - loss: 0.0606 - accuracy: 0.9784 - val_loss: 0.5254 - val_accuracy: 0.8862\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9778\n",
      "Epoch 94: loss did not improve from 0.06059\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.0643 - accuracy: 0.9778 - val_loss: 0.5145 - val_accuracy: 0.8898\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9784\n",
      "Epoch 95: loss did not improve from 0.06059\n",
      "782/782 [==============================] - 105s 135ms/step - loss: 0.0613 - accuracy: 0.9784 - val_loss: 0.4574 - val_accuracy: 0.8965\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9778\n",
      "Epoch 96: loss did not improve from 0.06059\n",
      "782/782 [==============================] - 103s 132ms/step - loss: 0.0625 - accuracy: 0.9778 - val_loss: 0.5921 - val_accuracy: 0.8743\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9794\n",
      "Epoch 97: loss improved from 0.06059 to 0.05839, saving model to model1.h5\n",
      "782/782 [==============================] - 104s 133ms/step - loss: 0.0584 - accuracy: 0.9794 - val_loss: 0.5359 - val_accuracy: 0.8885\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9787\n",
      "Epoch 98: loss did not improve from 0.05839\n",
      "782/782 [==============================] - 103s 131ms/step - loss: 0.0615 - accuracy: 0.9787 - val_loss: 0.4863 - val_accuracy: 0.8941\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9787\n",
      "Epoch 99: loss did not improve from 0.05839\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.0593 - accuracy: 0.9787 - val_loss: 0.5227 - val_accuracy: 0.8889\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9783\n",
      "Epoch 100: loss did not improve from 0.05839\n",
      "782/782 [==============================] - 102s 130ms/step - loss: 0.0609 - accuracy: 0.9783 - val_loss: 0.5452 - val_accuracy: 0.8846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a246f86d60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the previously trained model and continuing to train to imporove test accuracy.\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model1.h5')\n",
    "\n",
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# this time, we will train for 100 epochs\n",
    "\n",
    "model.fit(iterator_train, batch_size=256, epochs=100, callbacks=callbacks_list,validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1e142ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step - loss: 0.5452 - accuracy: 0.8847\n",
      "Test loss: 0.545221209526062\n",
      "Test accuracy: 0.8847000002861023\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d0f4f",
   "metadata": {},
   "source": [
    "- we have trained the model for 200 epochs in total\n",
    "- we have got the test accuracy of 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944c9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
